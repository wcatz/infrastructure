# Prometheus Helm Chart Values
# These values configure the Prometheus monitoring stack

server:
  enabled: true
  persistentVolume:
    enabled: true
    size: 8Gi
  retention: "15d"
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi

alertmanager:
  enabled: true
  persistentVolume:
    enabled: true
    size: 2Gi
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

  # Alertmanager configuration
  config:
    global:
      resolve_timeout: 5m
      # Slack API URL (configure via secret or environment variable)
      # slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      # PagerDuty API URL
      # pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

    # Routing tree
    route:
      # Root route
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'service', 'namespace']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      
      # Child routes
      routes:
        # Critical alerts to PagerDuty and Slack critical channel
        - match:
            severity: critical
          receiver: 'pagerduty-critical'
          continue: true
          group_wait: 10s
          repeat_interval: 5m
        
        # Warning alerts to Slack warning channel
        - match:
            severity: warning
          receiver: 'slack-warning'
          continue: false
          group_wait: 30s
          repeat_interval: 4h
        
        # Backup-related alerts (high priority)
        - match_re:
            alertname: '^(Velero|Backup).*'
          receiver: 'backup-alerts'
          continue: true
        
        # Certificate-related alerts
        - match_re:
            alertname: '^Certificate.*'
          receiver: 'security-alerts'
          continue: true

    # Inhibition rules (suppress certain alerts when others are firing)
    inhibit_rules:
      # Suppress warning if critical is firing for same alert
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'namespace', 'service']
      
      # Suppress backup old alerts if backup failed alert is firing
      - source_match:
          alertname: 'VeleroBackupFailed'
        target_match:
          alertname: 'VeleroBackupTooOld'
        equal: ['schedule']

    # Receivers configuration
    receivers:
      - name: 'default'
        # Default receiver - logs only or webhook
        # webhook_configs:
        #   - url: 'http://webhook-receiver/alerts'
        #     send_resolved: true

      - name: 'pagerduty-critical'
        # PagerDuty for critical alerts
        # Configure PagerDuty integration key as a secret
        pagerduty_configs:
          - routing_key: '<PAGERDUTY_INTEGRATION_KEY>'  # Replace with secret reference
            description: '{{ .GroupLabels.alertname }} - {{ .CommonAnnotations.summary }}'
            severity: 'critical'
            client: 'Prometheus AlertManager'
            client_url: 'http://prometheus.monitoring.svc.cluster.local'
            send_resolved: true
            # Use service_key for Events API v1 (legacy)
            # Use routing_key for Events API v2 (recommended)
        
        # Also send to Slack critical channel
        slack_configs:
          - api_url: '<SLACK_WEBHOOK_URL>'  # Replace with secret reference
            channel: '#alerts-critical'
            title: 'üö® Critical Alert'
            text: |
              *Alert:* {{ .GroupLabels.alertname }}
              *Severity:* {{ .CommonLabels.severity }}
              *Summary:* {{ .CommonAnnotations.summary }}
              *Description:* {{ .CommonAnnotations.description }}
              *Namespace:* {{ .CommonLabels.namespace }}
            send_resolved: true
            username: 'Prometheus'
            icon_emoji: ':prometheus:'
            color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'

      - name: 'slack-warning'
        # Slack for warning alerts
        slack_configs:
          - api_url: '<SLACK_WEBHOOK_URL>'  # Replace with secret reference
            channel: '#alerts-warning'
            title: '‚ö†Ô∏è  Warning Alert'
            text: |
              *Alert:* {{ .GroupLabels.alertname }}
              *Severity:* {{ .CommonLabels.severity }}
              *Summary:* {{ .CommonAnnotations.summary }}
              *Description:* {{ .CommonAnnotations.description }}
              *Namespace:* {{ .CommonLabels.namespace }}
            send_resolved: true
            username: 'Prometheus'
            icon_emoji: ':warning:'
            color: 'warning'

      - name: 'backup-alerts'
        # Dedicated channel for backup alerts
        slack_configs:
          - api_url: '<SLACK_WEBHOOK_URL>'  # Replace with secret reference
            channel: '#alerts-backup'
            title: 'üíæ Backup Alert'
            text: |
              *Alert:* {{ .GroupLabels.alertname }}
              *Severity:* {{ .CommonLabels.severity }}
              *Summary:* {{ .CommonAnnotations.summary }}
              *Description:* {{ .CommonAnnotations.description }}
              *Schedule:* {{ .CommonLabels.schedule }}
            send_resolved: true
            username: 'Velero'
            icon_emoji: ':floppy_disk:'
            color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        
        # Also send to PagerDuty if critical
        pagerduty_configs:
          - routing_key: '<PAGERDUTY_INTEGRATION_KEY>'
            description: 'Backup Alert: {{ .GroupLabels.alertname }}'
            severity: '{{ .CommonLabels.severity }}'
            send_resolved: true

      - name: 'security-alerts'
        # Security-related alerts (certificates, secrets, etc.)
        slack_configs:
          - api_url: '<SLACK_WEBHOOK_URL>'  # Replace with secret reference
            channel: '#alerts-security'
            title: 'üîí Security Alert'
            text: |
              *Alert:* {{ .GroupLabels.alertname }}
              *Severity:* {{ .CommonLabels.severity }}
              *Summary:* {{ .CommonAnnotations.summary }}
              *Description:* {{ .CommonAnnotations.description }}
              *Certificate:* {{ .CommonLabels.name }}
              *Namespace:* {{ .CommonLabels.namespace }}
            send_resolved: true
            username: 'cert-manager'
            icon_emoji: ':lock:'
            color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
    
    # Templates for alert messages
    templates:
      - '/etc/alertmanager/template/*.tmpl'

serverFiles:
  alerting_rules.yml:
    groups:
      - name: infrastructure
        interval: 30s
        rules:
          # Node/Pod availability alerts
          - alert: NodeDown
            expr: up{job="kubernetes-nodes"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.instance }} is down"
              description: "Node {{ $labels.instance }} has been down for more than 5 minutes"

          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) > 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently ({{ $value }} restarts/sec in last 15 minutes)"

          # Resource alerts
          - alert: HighCPUUsage
            expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on {{ $labels.instance }}"
              description: "CPU usage is above 80% for more than 10 minutes"

          - alert: HighMemoryUsage
            expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage on {{ $labels.instance }}"
              description: "Memory usage is above 90%"

          - alert: DiskSpaceLow
            expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Disk space low on {{ $labels.instance }}"
              description: "Disk usage is above 85% on {{ $labels.mountpoint }}"

          # Cloudflared tunnel alerts
          - alert: CloudflaredTunnelDown
            expr: up{job="cloudflared"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Cloudflared tunnel is down"
              description: "Cloudflared tunnel has been down for more than 2 minutes"

          - alert: CloudflaredHighErrorRate
            expr: rate(cloudflared_tunnel_response_errors_total[5m]) > 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High error rate on Cloudflared tunnel"
              description: "Cloudflared tunnel has high error rate for 5 minutes"

          # Velero backup alerts
          - alert: VeleroBackupFailed
            expr: velero_backup_failure_total > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Velero backup failed"
              description: "Backup {{ $labels.schedule }} has failed"

          - alert: VeleroBackupTooOld
            expr: time() - velero_backup_last_successful_timestamp{schedule!=""} > 86400
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Velero backup is too old"
              description: "Last successful backup for schedule {{ $labels.schedule }} was more than 24 hours ago"

          - alert: VeleroBackupPartialFailure
            expr: velero_backup_partial_failure_total > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Velero backup partially failed"
              description: "Backup {{ $labels.schedule }} has partial failures"

          - alert: VeleroRestoreFailed
            expr: velero_restore_failed_total > 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Velero restore failed"
              description: "Restore operation has failed"

          # cert-manager alerts
          - alert: CertificateExpiringSoon
            expr: certmanager_certificate_expiration_timestamp_seconds - time() < 604800
            for: 1h
            labels:
              severity: warning
            annotations:
              summary: "Certificate expiring soon"
              description: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expires in less than 7 days"

          - alert: CertificateExpiringSoonCritical
            expr: certmanager_certificate_expiration_timestamp_seconds - time() < 259200
            for: 1h
            labels:
              severity: critical
            annotations:
              summary: "Certificate expiring very soon"
              description: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expires in less than 3 days"

          - alert: CertificateNotReady
            expr: certmanager_certificate_ready_status == 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Certificate not ready"
              description: "Certificate {{ $labels.namespace }}/{{ $labels.name }} is not in ready state"

          - alert: CertificateRenewalFailed
            expr: increase(certmanager_certificate_renewal_total{status="failed"}[1h]) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Certificate renewal failed"
              description: "Certificate {{ $labels.namespace }}/{{ $labels.name }} renewal has failed"

          # Kubernetes alerts
          - alert: KubernetesPersistentVolumeFull
            expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes PersistentVolume is almost full"
              description: "PersistentVolume {{ $labels.persistentvolumeclaim }} is {{ $value }}% full"

          - alert: KubernetesDeploymentReplicasMismatch
            expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Deployment replicas mismatch"
              description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has {{ $value }} replicas available, expected {{ $labels.spec_replicas }}"

nodeExporter:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

pushgateway:
  enabled: false

kubeStateMetrics:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
